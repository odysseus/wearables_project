{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Test - Full Algorithm Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import mpld3\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization of the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the \n",
    "            reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    return data[2:]\n",
    "\n",
    "\n",
    "def LoadTroikaRefFile(ref_fl):\n",
    "    \"\"\"\n",
    "    Loads the BPM array from a reference (label) file\n",
    "    \n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        lbls = LoadTroikaRefFile(ref_fls[0])\n",
    "        \n",
    "    Args:\n",
    "        ref_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy array of BPM labels.\n",
    "    \"\"\"\n",
    "    ref = sp.io.loadmat(ref_fl)\n",
    "    return ref['BPM0']\n",
    "\n",
    "def LoadSample(idx, sigs, refs, fs=125, train=True):\n",
    "    \"\"\"\n",
    "    Loads and bandpasses a sample returning a dict of processed data\n",
    "    \n",
    "    Usage:\n",
    "        data_dict = LoadSample(8)\n",
    "        \n",
    "    Args:\n",
    "        idx: The numeric index of the sample.\n",
    "        sigs: List of Troika HR signal filenames\n",
    "        refs: List of Troika HR label filenames\n",
    "             \n",
    "    Return:\n",
    "        A dict containing various data points. All points in the dict have had\n",
    "        a bandpass filter applied. `ppgspec` and `accspec` are spectrograms for\n",
    "        the PPG signal and the acceleromter magnitude signal, respectively.\n",
    "    \"\"\"\n",
    "    ppg, accx, accy, accz = LoadTroikaDataFile(sigs[idx])\n",
    "    ts = np.arange(len(ppg)) / fs\n",
    "    \n",
    "    if train:\n",
    "        lbl = LoadTroikaRefFile(refs[idx])\n",
    "        lblhz = (lbl / 60.0)\n",
    "\n",
    "    bppg, baccx, baccy, baccz = bandpass(ppg), bandpass(accx), bandpass(accy), bandpass(accz)\n",
    "\n",
    "    accmag = np.sqrt(np.square(accx) + np.square(accy) + np.square(accz))\n",
    "    baccmag = bandpass(accmag)\n",
    "    \n",
    "    freqs, _, ppgspec = sp.signal.spectrogram(bppg, fs=fs, nperseg=fs*8, noverlap=fs*6)\n",
    "    _, _, accspec = sp.signal.spectrogram(baccmag, fs=fs, nperseg=fs*8, noverlap=fs*6)\n",
    "    \n",
    "    sample = {}\n",
    "    sample['index'] = idx\n",
    "    sample['ftlen'] = len(ppgspec.T)\n",
    "    sample['ppg'] = bppg\n",
    "    sample['accx'] = baccx\n",
    "    sample['accy'] = baccy\n",
    "    sample['accz'] = baccz\n",
    "    sample['accmag'] = baccmag\n",
    "    sample['freqs'] = freqs\n",
    "    sample['ppgspec'] = ppgspec\n",
    "    sample['accspec'] = accspec\n",
    "    \n",
    "    if train:\n",
    "        sample['labels'] = lbl\n",
    "        sample['labelhz'] = lblhz\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability. \n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and corresponding \n",
    "            reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse rate\n",
    "            error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "\n",
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an aggregate error metric.\n",
    "\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "    \n",
    "    for idx in range(len(data_fls)):\n",
    "        data = LoadSample(idx, data_fls, ref_fls)\n",
    "        # Run the pulse rate algorithm on each trial in the dataset\n",
    "        errors, confidence = RunPulseRateAlgorithm(data)\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "        # Compute aggregate error metric\n",
    "        \n",
    "    errs = np.hstack(errs)\n",
    "    confs = np.hstack(confs)\n",
    "    return AggregateErrorMetric(errs, confs)\n",
    "\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):\n",
    "    \"\"\"\n",
    "    Runs the algorithm on a single sample and returns per-sample errors and confidence\n",
    "    \n",
    "    Args:\n",
    "        data: A data dict for the sample as returned by LoadSample()\n",
    "        \n",
    "    Returns:\n",
    "        A 2-tuple of (errors, confidence) for each 2s slice of the HR signal\n",
    "    \"\"\"\n",
    "    # A small hack to avoid undoing the changes I made to the original function signature\n",
    "    data = LoadSample(0, [data_fl], [ref_fl])\n",
    "    \n",
    "    # Compute pulse rate estimates and estimation confidence.\n",
    "    predshz = predict_series(data)\n",
    "    errors = np.abs((predshz * 60.0) - data['labels'])\n",
    "    confidence = confidence_series(data, predshz)\n",
    "\n",
    "    # Return per-estimate absolute error and confidence as a 2-tuple of numpy arrays.\n",
    "    return errors.reshape(-1,), confidence.reshape(-1,)\n",
    "\n",
    "def bandpass(sig):\n",
    "    \"\"\"\n",
    "    Applies a bandpass signal relevant to heart rate data to the input signal.\n",
    "    Wrapper function for scipy.signal.butter and scipy.signal.filtfilt\n",
    "    applied in that order.\n",
    "    \n",
    "    Usage:\n",
    "        filtered_signal = bandpass(input_signal)\n",
    "        \n",
    "    Args:\n",
    "        sig: Input signal, any value that scipy.signal.butter can take\n",
    "        \n",
    "    Returns:\n",
    "        The bandpassed and filtered signal\n",
    "    \"\"\"\n",
    "    lohz, hihz = 40/60.0, 240/60.0\n",
    "    b, a = sp.signal.butter(3, [lohz, hihz], btype='bandpass', fs=125)\n",
    "    return sp.signal.filtfilt(b, a, sig)\n",
    "\n",
    "def hr_peaks(sig, hthresh=0.05, dist=2):\n",
    "    \"\"\"\n",
    "    Wrapper function that finds peaks using scipy.signal.peaks \n",
    "    with values relevant to heart rate data.\n",
    "    \n",
    "    Usage:\n",
    "        peak_indices = hr_peaks(sig)\n",
    "        \n",
    "    Args:\n",
    "        sig: np.array containing the input signal\n",
    "        \n",
    "    Returns:\n",
    "        Array of indices locating peaks in the signal\n",
    "    \"\"\"\n",
    "    maxf = np.max(sig)\n",
    "    minh = maxf * hthresh\n",
    "    peaks, _  = sp.signal.find_peaks(sig, height=minh, distance=dist)\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "\n",
    "def predict(data,\n",
    "            idx,\n",
    "            pred_type='compensated',\n",
    "            epsilon=0.10, \n",
    "            harmonics=lambda x: [x/2, x, x*2],\n",
    "            peakthresh=0.05,\n",
    "            peakdist=2):\n",
    "    \"\"\"\n",
    "    Predicts the HR value for a single data point.\n",
    "    \n",
    "    Args:\n",
    "        - data: The data dict as returned by LoadSample\n",
    "        - idx: The index for the slice of interest within the data spectrograms\n",
    "        - pred_type: 'naive' to predict the highest magnitude signal,\n",
    "        'compensated' to adjust via the accelerometer data.\n",
    "        - epsilon: How close an observation must be to an accelerometer peak\n",
    "        for us to rule it out as a motion artifact and not a true signal.\n",
    "        - harmonics: A lambda that returns an array of harmonics that we use to\n",
    "        define accelerometer peaks. `x` will be the fundamental frequency of\n",
    "        the accelerometer magnitude.\n",
    "        - peakthresh: How tall a peak needs to be to be considered a peak, a value\n",
    "        between 0.0 and 1.0 where 1.0 is the highest peak. EG: 0.5 means the peak\n",
    "        must be at least half the height of the highest peak.\n",
    "        - peakdist: How many samples before we can find another peak. Note that the\n",
    "        Fourier transform leaves few samples so values above 3 will exclude a large\n",
    "        range of the signal\n",
    "        \n",
    "    Returns:\n",
    "        A predicted value expressed as Hz\n",
    "    \"\"\"\n",
    "    \n",
    "    ppgsig = data['ppgspec'].T[idx]\n",
    "    accsig = data['accspec'].T[idx]\n",
    "    freqs = data['freqs']\n",
    "    \n",
    "    if pred_type == 'naive':\n",
    "        return freqs[np.argmax(ppgsig)]\n",
    "    else:\n",
    "        ppgpeakidx = hr_peaks(ppgsig, hthresh=peakthresh, dist=peakdist)\n",
    "        accpeakidx = hr_peaks(accsig, hthresh=peakthresh, dist=peakdist)\n",
    "\n",
    "        ppgpeakhz = freqs[ppgpeakidx]\n",
    "        accpeakhz = freqs[accpeakidx]\n",
    "\n",
    "        if len(ppgpeakhz) == 0:\n",
    "            return freqs[np.argmax(ppgsig)]\n",
    "        elif len(ppgpeakhz) == 1:\n",
    "            return ppgpeakhz[0]\n",
    "        else:\n",
    "            candidates = ppgpeakhz\n",
    "            maxacchz = accpeakhz[np.argmax(accsig[accpeakidx])]\n",
    "            excludehz = np.array(harmonics(maxacchz))\n",
    "            mask = (candidates == candidates)\n",
    "\n",
    "            for fq in excludehz:\n",
    "                m = np.abs(candidates - fq) > epsilon\n",
    "                mask = mask & m\n",
    "\n",
    "            filtered = candidates[mask]\n",
    "            if len(filtered) == 0:\n",
    "                return freqs[np.argmax(ppgsig)]\n",
    "            if len(filtered) == 1:\n",
    "                return filtered[0]\n",
    "            else:\n",
    "                peakmags = ppgsig[ppgpeakidx[mask]]\n",
    "                return filtered[np.argmax(peakmags)]\n",
    "            \n",
    "    return None\n",
    "        \n",
    "        \n",
    "def predict_series(data,\n",
    "                   pred_type='compensated',\n",
    "                   epsilon=0.10,\n",
    "                   harmonics=lambda x: [x/2, x, x*2],\n",
    "                   peakthresh=0.05,\n",
    "                   peakdist=2):\n",
    "    \"\"\"\n",
    "    Runs predictions on every slice of the spectrogram contained in a data dict\n",
    "    \n",
    "    Args:\n",
    "        Refer to predict(), all args are passed through to it\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of  predictions. Shape (n,1) where n is the length of the spectrogram\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array([predict(data,\n",
    "                    i,\n",
    "                    pred_type=pred_type,\n",
    "                    epsilon=epsilon,\n",
    "                    harmonics=harmonics,\n",
    "                    peakthresh=peakthresh,\n",
    "                    peakdist=peakdist)\n",
    "            for i in range(data['ftlen'])]).reshape(-1,1)\n",
    "\n",
    "\n",
    "def confidence_series(data, preds):\n",
    "    \"\"\"\n",
    "    Returns the confidence level in each prediction\n",
    "    \n",
    "    Args:\n",
    "        - data: The data dict as returned by LoadSample()\n",
    "        - preds: An array of predictions, as returned by predict_series()\n",
    "        \n",
    "    Returns:\n",
    "        - An array of per-sample confidence levels of the shape (n,1) where\n",
    "        n is the number of predictions/labels, matches length of the spectrogram\n",
    "    \"\"\"\n",
    "    ppgspec = data['ppgspec']\n",
    "    freqs = data['freqs']\n",
    "    \n",
    "    confidences = []\n",
    "    for i in range(len(preds)):\n",
    "        ppgsig = ppgspec.T[i]\n",
    "        pred = preds[i]\n",
    "        predidx = np.argmin(np.abs(pred - freqs))\n",
    "        if predidx == 0:\n",
    "            checkfreqs = [predidx, predidx+1]\n",
    "        elif predidx == len(preds)-1:\n",
    "            checkfreqs = [predidx-1, predidx]\n",
    "        else:\n",
    "            checkfreqs = [predidx-1, predidx, predidx+1]\n",
    "        \n",
    "        checkintensity = np.sum(ppgsig[checkfreqs])\n",
    "        totalintensity = np.sum(ppgsig)\n",
    "        confidences.append(checkintensity / totalintensity)\n",
    "        \n",
    "    return np.array(confidences).reshape(-1,1)\n",
    "\n",
    "\n",
    "def mae(labels, preds):\n",
    "    \"\"\"Returns the mean average error between the labels and predictions\"\"\"\n",
    "    return np.mean(np.abs(preds - labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='unit_test.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
